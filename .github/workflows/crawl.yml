name: Daily Web Crawl

on:
  schedule:
    - cron: '0 2 * * *' # Daily at 2 AM UTC
  workflow_dispatch:
    inputs:
      crawler:
        description: 'Specific crawler module to run (e.g. example). Empty runs all.'
        required: false
        default: ''
      debug:
        description: 'Enable debug mode'
        required: false
        default: 'false'
      run_date:
        description: 'UTC date YYYY-MM-DD (defaults to today)'
        required: false
        default: ''

concurrency:
  group: crawler-data
  cancel-in-progress: false

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 180

    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('crawlers/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r crawlers/requirements.txt

      - name: Prepare data worktree (data branch)
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # Create a worktree checked out at ./data-worktree
          # If the data branch doesn't exist yet, create it orphaned.
          if git ls-remote --exit-code --heads origin data >/dev/null 2>&1; then
            git fetch origin data
            git worktree add -B data data-worktree origin/data
          else
            git worktree add -b data data-worktree
            pushd data-worktree
            git rm -rf . >/dev/null 2>&1 || true
            mkdir -p data
            echo "# Data branch" > README.md
            git add README.md data
            git commit -m "Initialize data branch"
            popd
          fi

      - name: Archive previous latest
        run: |
          python - <<'PY'
          from pathlib import Path
          from utils.data_rotation import archive_previous_latest

          data_root = Path('data-worktree/data')
          res = archive_previous_latest(data_root)
          print(f"archived={res.archived} path={res.archived_path}")
          PY

      - name: Run crawler(s)
        env:
          CRAWLER: ${{ github.event.inputs.crawler }}
          DEBUG: ${{ github.event.inputs.debug }}
          RUN_DATE: ${{ github.event.inputs.run_date }}
        run: |
          args=""
          if [ -n "$CRAWLER" ]; then args="$args --crawler $CRAWLER"; fi
          if [ "$DEBUG" = "true" ]; then args="$args --debug"; fi
          if [ -n "$RUN_DATE" ]; then args="$args --run-date $RUN_DATE"; fi

          python main.py $args --out data-worktree/data

      - name: Commit and push data branch
        run: |
          pushd data-worktree

          # Ensure we only commit data outputs
          # Split add commands so if archive doesn't exist (first run), latest is still added
          git add -f -A data/latest || true
          git add -f -A data/archive || true

          if git diff --cached --quiet; then
            echo "No data changes to commit"
            popd
            exit 0
          fi

          stamp=$(date -u +"%Y-%m-%d")
          git commit -m "Daily crawl data: $stamp" || true
          git push origin data

          popd

      - name: Upload logs/data snapshot (debugging)
        uses: actions/upload-artifact@v4
        with:
          name: crawl-snapshot-${{ github.run_id }}
          path: |
            data-worktree/data/latest/*.json
            data-worktree/data/latest/*.jsonl
          retention-days: 7
